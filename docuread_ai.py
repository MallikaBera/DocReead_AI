# -*- coding: utf-8 -*-
"""HelpMate_AI.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/113GOZh8Q0CdnaOLij6HMIuEfQhRzKNcu

# Mr.HelpMate AI

## Project Overview:
This project aims to develop a robust Retrieval-Augmented Generation (RAG) system designed to efficiently extract and provide accurate answers from life insurance policy documents. By leveraging advanced retrieval and generative AI techniques, the system will enable users to query a policy document and receive precise, context-aware responses, reducing the need for manual searches and improving accessibility.

## Objective:
The goal of the project will be to build a robust generative search system capable of effectively and accurately answering questions from a policy document.We will be using a single long life insurance policy document for this project.

### Installing required libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install --force-reinstall --no-cache-dir tenacity --user
# !pip install "langchain-ibm==0.1.7" --user
# !pip install "langchain-community==0.2.1" --user
# !pip install "langchain-experimental==0.0.59" --user
# !pip install "langchainhub==0.1.17" --user
# !pip install "langchain==0.2.1" --user
# !pip install "pypdf==4.2.0" --user
# !pip install "chromadb == 0.4.24" --user
# !pip install -qU pypdf
#

# !pip install chromadb

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-openai langchain-community

# !pip install --upgrade --force-reinstall --no-cache-dir langchain langchain-openai langchain-core --user

!pip install sentence-transformers --user

"""### Import Dataset

"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/helpmate')
!ls

# Set the API key
filepath = "/content/drive/MyDrive/helpmate/OPENAI_API_Key.txt"

with open(filepath, "r") as f:
  api_key = ' '.join(f.readlines())

"""The following will construct a GPT 3.5 model object:

"""

import getpass
import os

if not os.environ.get("OPENAI_API_KEY"):
  os.environ["OPENAI_API_KEY"] = api_key


# Import ChatOpenAI directly
from langchain_openai import ChatOpenAI

# Initialize the ChatOpenAI model
llm = ChatOpenAI(model_name="gpt-3.5-turbo")

"""Let's use a simple example to test the model generate some text:

"""

from langchain_core.messages import HumanMessage

response = llm([HumanMessage(content="Generate an insightful response.")])
print(response)

"""### Embedding Layer

We would load the insurance PDF as LangChain document.
"""

# Load and parse the given pdf document
from langchain_community.document_loaders import PyPDFLoader

file_path = '/content/drive/MyDrive/helpmate/Principal-Sample-Life-Insurance-Policy.pdf'

loader = PyPDFLoader(file_path)

# Returns a list of Document objects-- one per page-- containing a single string of the page's text in the Document's page_content attribute
pages = []
async for page in loader.alazy_load():
    pages.append(page)

len(pages)

# Check page content
print(f"{pages[1].metadata}\n")
print(pages[1].page_content)

"""As we can see there are many pages with unuseful information such as 'This page left blank intentionally'. We need to remove them. We will clean the data the removing pages where content is very less"""

# Remove pages with irrelevant information or very minimal information
def filter_pdf_pages(docs, min_text_length=35):
  filtered_docs = [doc for doc in docs if len(doc.page_content.strip()) > min_text_length]

  return filtered_docs

# Filterting the pdf
filtered_docs = filter_pdf_pages(pages)

# Check if the empty pages are removed
print(filtered_docs[1].page_content)

len(filtered_docs)

"""Now we will split the data into smaller chunks. For our analysis we have used two chunking methods:

1. Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.

2. RecursiveCharacterTextSplitter : Creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity.

"""

# CharacterTextSplitter
from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator="\n", chunk_size=1000, chunk_overlap=0
)
texts = text_splitter.split_documents(filtered_docs)

texts[2]

len(texts) , len(filtered_docs)

# RecursiveCharacterTextSplitter
from langchain.text_splitter import RecursiveCharacterTextSplitter

def split_into_chunks(text_pages, chunk_size=500, overlap=100):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
    # Extract the page_content from each Document object
    text_contents = [doc.page_content for doc in text_pages]
    chunks = text_splitter.create_documents(text_contents)
    return [chunk.page_content for chunk in chunks]

texts_recursive = split_into_chunks(filtered_docs)

texts_recursive[2]

len(texts_recursive) , len(pages)

"""Let us now remove the blank spaces before we do the embedding."""

def remove_ws(d):
    text = d.page_content.replace('\n','')
    d.page_content = text
    return d

# applied on the CharacterTextSplitter
docs = [remove_ws(t) for t in texts]

"""### The Search Layer

We have identified some queries to highlight the key sections of our document:

1. What is the eligibility of Member Life Insurance?
2. What is the maximum claim amount for Member Accidental Death and Dismemberment Insurance?
3. What happens when the policy member fails to pay the premium?

We will be embedding Queries & Search using ChromaDB
"""

from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# Embed docs and store in ChromaDB
vectorstore = Chroma.from_documents(docs, embedding=OpenAIEmbeddings())

"""We can also use embeddings with RecursiveEmbedding"""

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")
embedded_chunks = [model.encode(chunk) for chunk in texts_recursive]

len(embedded_chunks)

# Implementing Cache Mechanism
from langchain_community.cache import InMemoryCache
from langchain_core.globals import set_llm_cache

cache = InMemoryCache()
set_llm_cache(cache)

"""The first time, it is not yet in cache, so it should take longer"""

# Clear all entries
cache.clear()

"""#### Querying

Searching queries using vectorstore with CharacterTextSplitter:
"""

# Commented out IPython magic to ensure Python compatibility.
# # Search using queries
# %%time
# query1 = "What is the eligibility of Member Life Insurance?"
# query1_response = vectorstore.similarity_search(query1)
# print("Query 1 : What is the eligibility of Member Life Insurance?")
# print("Response:")
# print(query1_response[0].page_content)
# print('\n')
# 
# query2 = "What is the maximum claim amount for Member Accidental Death and Dismemberment Insurance?"
# query2_response = vectorstore.similarity_search(query2)
# print("Query 2 : What is the maximum claim amount for Member Accidental Death and Dismemberment Insurance?")
# print("Response:")
# print(query2_response[0].page_content)
# print('\n')
# 
# query3 = "What happens when the policy member fails to pay the premium?"
# query3_response = vectorstore.similarity_search(query3)
# print("Query 3 : What happens when the policy member fails to pay the premium?")
# print("Response:")
# print(query3_response[0].page_content)
# print('\n')

"""The second time it is, so it goes faster"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# query1 = "What is the eligibility of Member Life Insurance?"
# query1_response = vectorstore.similarity_search(query1)
# print("Query 1 : What is the eligibility of Member Life Insurance?")
# print("Response:")
# print(query1_response[0].page_content)
# print('\n')
# 
# query2 = "What is the maximum claim amount for Member Accidental Death and Dismemberment Insurance?"
# query2_response = vectorstore.similarity_search(query2)
# print("Query 2 : What is the maximum claim amount for Member Accidental Death and Dismemberment Insurance?")
# print("Response:")
# print(query2_response[0].page_content)
# print('\n')
# 
# query3 = "What happens when the policy member fails to pay the premium?"
# query3_response = vectorstore.similarity_search(query3)
# print("Query 3 : What happens when the policy member fails to pay the premium?")
# print("Response:")
# print(query3_response[0].page_content)
# print('\n')

"""#### Retriever
Once the documents are embedded, we will create a retriever for querying:
"""

retriver_char = vectorstore.as_retriever()

"""#### Re-Ranking

Now we want to use reranking mechanism. We will use transformer models to re-rank the retrieved results:
"""

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

# Defining the Ranking Mechanism for the Embedded texts
def pdf_encoder(model_name, query, chunks):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)


    inputs = [query + " [SEP] " + chunk for chunk in chunks]
    tokenized = tokenizer(inputs, padding=True, truncation=True, return_tensors="pt")

    with torch.no_grad():
        scores = model(**tokenized).logits.squeeze()

     # Return a list of tuples containing (chunk, score)
    sorted_chunks_with_scores = [(chunk, score) for score, chunk in sorted(zip(scores, chunks), reverse=True)]
    return sorted_chunks_with_scores

# Query 1 : Retrieving the top 3 results from Search layer
ranked_chunks = pdf_encoder("cross-encoder/ms-marco-MiniLM-L6-v2", query1, texts_recursive)

# Display Query
print("Query 1 :",query1)
print("Response:")

# Display top 3 ranked results
for doc, score in ranked_chunks[:3]:
    # Access the string content directly from 'doc'
    print(f"Score: {score:.2f} | {doc[:200]}...")
    print('\n\n')

# Query 2 : Retrieving the top 3 results from Search layer
ranked_chunks = pdf_encoder("cross-encoder/ms-marco-MiniLM-L6-v2", query2, texts_recursive)

# Display Query
print("Query 2 :",query2)
print("Response:")

# Display top 3 ranked results
for doc, score in ranked_chunks[:3]:
    # Access the string content directly from 'doc'
    print(f"Score: {score:.2f} | {doc[:200]}...")
    print('\n\n')

# Query 3 : Retrieving the top 3 results from Search layer
ranked_chunks = pdf_encoder("cross-encoder/ms-marco-MiniLM-L6-v2", query3, texts_recursive)

# Display Query
print("Query 3 :",query3)
print("Response:")

# Display top 3 ranked results
for doc, score in ranked_chunks[:3]:
    # Access the string content directly from 'doc'
    print(f"Score: {score:.2f} | {doc[:200]}...")
    print('\n\n')

# Query 3 : Retrieving the top 3 results from Search layer
query = "What exclusions apply to medical coverage?"
ranked_chunks = pdf_encoder("cross-encoder/ms-marco-MiniLM-L6-v2", query, texts_recursive)

# Display Query
print("Query 3 :","Does the policy cover accidental injuries?")
print("Response:")

# Display top 3 ranked results
for doc, score in ranked_chunks[:3]:
    # Access the string content directly from 'doc'
    print(f"Score: {score:.2f} | {doc[:200]}...")
    print('\n\n')

"""### Generation Layer
Here we will design the final prompt. Let us also use Few-shot examples to improve the LLM’s ability to generate accurate and well-structured responses and reduce hallucinations by reinforcing correct answers
"""

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage

from langchain_openai import ChatOpenAI

def generate_response(query):
    # Define query and ranked results
    model_name = "cross-encoder/ms-marco-MiniLM-L6-v2"
    # Rank the retrieved chunks
    ranked_chunks = pdf_encoder(model_name, query, texts_recursive)

    # Limit to the top N chunks (e.g., top 3)
    top_n_ranked_chunks = ranked_chunks[:3]

    # Format final prompt with structured response guidelines
    final_prompt = f"""
    You are an AI assistant designed to answer questions based on retrieved text chunks. Your goal is to generate a well-structured response that accurately synthesizes information from multiple sources.

    ### Instructions:
    - Read and analyze the retrieved chunks carefully.
    - Output should be the result with highest score predicted.
    - If contradictory information is present, reason through the differences.
    - Provide responses in a **concise yet detailed manner**.
    - Format your response with headings when necessary.
    - If relevant data is missing, acknowledge the limitation.
    - Maintain an **informative and professional tone**.

    ### Few-Shot Examples:
    Example 1:
    User Question: "Does the policy cover accidental injuries?"
    Retrieved Chunks:
    - "Score: 1.12 | Written proof of ADL Disability or Total Disability must be sent to The Principal within one year of the date ADL Disability or Total Disability begins."
    - "Score: -0.61 | a. Member Life Insurance
      The total volume of insurance in force will be divided by 1,000.  The result will then be
      multiplied by the premium rate then in effect. "

    Expected Output:
    "Written proof of ADL Disability or Total Disability must be sent to The Principal within one year of the date ADL Disability or Total Disability begins."

    Example 2:
    User Question: ""What exclusions apply to medical coverage?"
    Retrieved Chunks:
    - "Score: 1.65 | Written proof of ADL Disability or Total Disability must be sent to The Principal within
      one year of the date ADL Disability or Total Disability begins.  Further proof that ADL
      Disability or Total D..."
    - "Score: 0.19 | (1) the date the Member is eligible, if the request is made on or before that date; or
    (2) the date of the Member's request, if the request is made within 31 days after the date
    the Member is eligib...
    "

    Expected Output:
    "Written proof of ADL Disability or Total Disability must be sent to The Principal within
      one year of the date ADL Disability or Total Disability begins.  Further proof that ADL
      Disability or Total D..."

    #### **User Query:**
    {query}

    #### **Ranked Retrieved Chunks (Highest relevance first):**
    {chr(10).join([f"- {chunk[0]} (Score: {chunk[1]:.3f})" for chunk in top_n_ranked_chunks])}

    ### **Response:**

    """
    return final_prompt

!pip install streamlit ngrok

import streamlit as st
# Streamlit UI
st.title("AI-Powered Document Retrieval & Response Generation")

"""Now let's simulate a chat experience with the bot:"""

llm = ChatOpenAI(model_name="gpt-3.5-turbo")
query = st.text_input("Enter your query:")
if st.button("Generate Response"):
  response = llm.invoke(generate_response(query))
  st.write(response)

!streamlit run app.py &

from pyngrok import ngrok

public_url = ngrok.connect(port=8501)
print(f"Access your app here: {public_url}")
